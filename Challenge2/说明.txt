*Bagging：
n_estimators为基础估计器的数量，越多效果越好，但占用的资源也越多，增加到一定量之后，回归效果逐渐不再明显提升
选择兼顾时间和精度，选择200

*CART：
选择最优深度

*Random Forest
n_estimators为树的个数，同Bagging

*L2Boosting：
n_estimators选择同上
学习率过大会导致无法找到最优解，过小会导致陷入局部最优以及增加时间
#n_iter_no_change, validation_fraction 可以帮助在分数不再提高之后终止训练，减少训练时间

*SVM（这里采用的是SVR，支持向量回归）
首先选择核函数，发现linear误差最小
通过改变C和epsilon的值发现，C值越大，epsilon值越小，回归的效果越好，
在C足够大，epsilon足够小时，rbf核函数的误差更小
当到达最终选用的值（C=100000, epsilon = 0.00001）后，继续增大C和减小epsilon效果提升不明显，且计算时间明显变久

*神经网络（BP）
训练次数（epoch=200）过多会导致过拟合，误差变大；学习率比较敏感，过大会导致无法找到最优解，过小会导致陷入局部最优
批尺寸（batch_size）和权重衰减（weight_decay）的影响相对较小，增大批尺寸减小了误差，权重衰减的设置可以减少过拟合，但在本例中增加权重衰减增加了误差
两张图分别说明了，epoch=200时发现尽管训练集的误差在减小，但验证集的误差越来越大，这是过拟合导致的，还有一张表示正常的训练过程

最终结果发现，SVM的Kaggle得分比较差，这可能是由于C值过大，epsilon值过小导致的过拟合